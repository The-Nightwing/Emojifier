{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary={\n",
    "    \"0\":\"\\u2764\\uFE0F\",\n",
    "    \"1\":\":baseball:\",\n",
    "    \"2\":\":grinning_face_with_big_eyes:\",\n",
    "    \"3\":\":disappointed_face:\",\n",
    "    \"4\":\":fork_and_knife:\",\n",
    "    \"5\":\":hundred_points:\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ù§Ô∏è\n",
      "‚öæ\n",
      "üòÉ\n",
      "üòû\n",
      "üç¥\n",
      "üíØ\n"
     ]
    }
   ],
   "source": [
    "for e in emoji_dictionary.values():\n",
    "    print(emoji.emojize(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train_emoji.csv',header=None)\n",
    "test=pd.read_csv('test_emoji.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>never talk to me again</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am proud of your achievements</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is the worst day in my life</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Miss you so much</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>food is life</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0  1   2     3\n",
       "0           never talk to me again  3 NaN   NaN\n",
       "1  I am proud of your achievements  2 NaN   NaN\n",
       "2   It is the worst day in my life  3 NaN   NaN\n",
       "3                 Miss you so much  0 NaN   [0]\n",
       "4                     food is life  4 NaN   NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 4)\n"
     ]
    }
   ],
   "source": [
    "#lets print sentences with emojis\n",
    "data=train.values\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train[0]\n",
    "Y_train=train[1]\n",
    "\n",
    "X_test=test[0]\n",
    "Y_test=test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never talk to me again üòû\n",
      "I am proud of your achievements üòÉ\n",
      "It is the worst day in my life üòû\n",
      "Miss you so much ‚ù§Ô∏è\n",
      "food is life üç¥\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(X_train[i],emoji.emojize(emoji_dictionary[str(Y_train[i])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<b>Step-3: Converting Sentences into Embeddings</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('glove.6B.50d.txt',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "cnt=0\n",
    "for line in f:\n",
    "    values=line.split()\n",
    "    word=values[0]\n",
    "    coefs=np.asarray(values[1:],dtype='float')\n",
    "    embeddings_index[word]=coefs #made dictionary for every 6 billion word\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim=embeddings_index[\"eat\"].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step-4 Converting Sentences into vectors.(Embedding layer output) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_output(X):\n",
    "    maxLen=10\n",
    "    embedding_out=np.zeros((X.shape[0],maxLen,emb_dim))\n",
    "    \n",
    "    for ix in range(X.shape[0]):\n",
    "        X[ix]=X[ix].split() #splitting ixth sentence\n",
    "        for ij in range(len(X[ix])):\n",
    "            #go to every word in the current (ix) sentence\n",
    "            \n",
    "            try:\n",
    "                embedding_out[ix][ij] = embeddings_index[X[ix][ij].lower()]\n",
    "            except:\n",
    "                embedding_out[ix][ij] = np.zeros((50,))\n",
    "                \n",
    "    return embedding_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\verma\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "embeddings_matrix_train=embedding_output(X_train)\n",
    "embeddings_matrix_test=embedding_output(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these both matrix will be three dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=to_categorical(Y_train,num_classes=5)\n",
    "Y_test=to_categorical(Y_test,num_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step-5: RNN/LSTM Architecuture </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 10, 64)            29440     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 62,789\n",
      "Trainable params: 62,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(LSTM(64,input_shape=(10,50),return_sequences=True)) #return sequence for giving to the lstm which is going to be stacked#10 word sentence with 50 dimensional-each word\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(64,return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.6177 - acc: 0.1562\n",
      "Epoch 00001: val_loss improved from inf to 1.62885, saving model to model.hdf5\n",
      "4/4 [==============================] - 2s 461ms/step - loss: 1.6141 - acc: 0.1780 - val_loss: 1.6288 - val_acc: 0.1429\n",
      "Epoch 2/40\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.5281 - acc: 0.3305\n",
      "Epoch 00002: val_loss did not improve from 1.62885\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 1.5281 - acc: 0.3305 - val_loss: 1.6498 - val_acc: 0.2143\n",
      "Epoch 3/40\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 1.4886 - acc: 0.3750\n",
      "Epoch 00003: val_loss did not improve from 1.62885\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 1.5272 - acc: 0.3136 - val_loss: 1.6773 - val_acc: 0.2143\n",
      "Epoch 4/40\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 1.4724 - acc: 0.3750\n",
      "Epoch 00004: val_loss did not improve from 1.62885\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 1.5039 - acc: 0.3305 - val_loss: 1.6514 - val_acc: 0.2143\n",
      "Epoch 5/40\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.4321 - acc: 0.4068\n",
      "Epoch 00005: val_loss improved from 1.62885 to 1.60660, saving model to model.hdf5\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 1.4321 - acc: 0.4068 - val_loss: 1.6066 - val_acc: 0.1429\n",
      "Epoch 6/40\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.4193 - acc: 0.4153\n",
      "Epoch 00006: val_loss improved from 1.60660 to 1.53808, saving model to model.hdf5\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.4193 - acc: 0.4153 - val_loss: 1.5381 - val_acc: 0.4286\n",
      "Epoch 7/40\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.3611 - acc: 0.5000\n",
      "Epoch 00007: val_loss improved from 1.53808 to 1.46980, saving model to model.hdf5\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 1.3611 - acc: 0.5000 - val_loss: 1.4698 - val_acc: 0.2857\n",
      "Epoch 8/40\n",
      "4/4 [==============================] - ETA: 0s - loss: 1.2769 - acc: 0.5085\n",
      "Epoch 00008: val_loss improved from 1.46980 to 1.39949, saving model to model.hdf5\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 1.2769 - acc: 0.5085 - val_loss: 1.3995 - val_acc: 0.2857\n",
      "Epoch 9/40\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 1.1279 - acc: 0.5312\n",
      "Epoch 00009: val_loss improved from 1.39949 to 1.33233, saving model to model.hdf5\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 1.1804 - acc: 0.5593 - val_loss: 1.3323 - val_acc: 0.4286\n",
      "Epoch 10/40\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 1.0708 - acc: 0.5312\n",
      "Epoch 00010: val_loss improved from 1.33233 to 1.25447, saving model to model.hdf5\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 1.0906 - acc: 0.6017 - val_loss: 1.2545 - val_acc: 0.5714\n",
      "Epoch 11/40\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 1.1251 - acc: 0.6250\n",
      "Epoch 00011: val_loss improved from 1.25447 to 1.22616, saving model to model.hdf5\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 1.0066 - acc: 0.6441 - val_loss: 1.2262 - val_acc: 0.5000\n",
      "Epoch 12/40\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.9031 - acc: 0.6525\n",
      "Epoch 00012: val_loss improved from 1.22616 to 1.17336, saving model to model.hdf5\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.9031 - acc: 0.6525 - val_loss: 1.1734 - val_acc: 0.5714\n",
      "Epoch 13/40\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6922 - acc: 0.7500\n",
      "Epoch 00013: val_loss improved from 1.17336 to 1.16543, saving model to model.hdf5\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.8440 - acc: 0.7119 - val_loss: 1.1654 - val_acc: 0.5000\n",
      "Epoch 14/40\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.7626 - acc: 0.7373\n",
      "Epoch 00014: val_loss improved from 1.16543 to 1.07022, saving model to model.hdf5\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.7626 - acc: 0.7373 - val_loss: 1.0702 - val_acc: 0.5000\n",
      "Epoch 15/40\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5989 - acc: 0.8438\n",
      "Epoch 00015: val_loss did not improve from 1.07022\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.5955 - acc: 0.8305 - val_loss: 1.1000 - val_acc: 0.6429\n",
      "Epoch 16/40\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5210 - acc: 0.8750\n",
      "Epoch 00016: val_loss did not improve from 1.07022\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5764 - acc: 0.8390 - val_loss: 1.3092 - val_acc: 0.5000\n",
      "Epoch 17/40\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4863 - acc: 0.8390\n",
      "Epoch 00017: val_loss improved from 1.07022 to 0.84740, saving model to model.hdf5\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4863 - acc: 0.8390 - val_loss: 0.8474 - val_acc: 0.7143\n",
      "Epoch 18/40\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5770 - acc: 0.7812\n",
      "Epoch 00018: val_loss did not improve from 0.84740\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.5275 - acc: 0.8136 - val_loss: 0.9055 - val_acc: 0.6429\n",
      "Epoch 19/40\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5215 - acc: 0.7500\n",
      "Epoch 00019: val_loss did not improve from 0.84740\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.4429 - acc: 0.8220 - val_loss: 1.1819 - val_acc: 0.6429\n",
      "Epoch 20/40\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.4187 - acc: 0.8729\n",
      "Epoch 00020: val_loss did not improve from 0.84740\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.4187 - acc: 0.8729 - val_loss: 0.9073 - val_acc: 0.7143\n",
      "Epoch 21/40\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2964 - acc: 0.8814\n",
      "Epoch 00021: val_loss did not improve from 0.84740\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.2964 - acc: 0.8814 - val_loss: 0.8996 - val_acc: 0.7143\n",
      "Epoch 22/40\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3072 - acc: 0.9375\n",
      "Epoch 00022: val_loss did not improve from 0.84740\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.3101 - acc: 0.9153 - val_loss: 0.9786 - val_acc: 0.7143\n",
      "Epoch 23/40\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3103 - acc: 0.9062\n",
      "Epoch 00023: val_loss did not improve from 0.84740\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.3496 - acc: 0.8898 - val_loss: 1.2792 - val_acc: 0.5714\n",
      "Epoch 24/40\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4637 - acc: 0.8438\n",
      "Epoch 00024: val_loss improved from 0.84740 to 0.81966, saving model to model.hdf5\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2850 - acc: 0.9068 - val_loss: 0.8197 - val_acc: 0.7143\n",
      "Epoch 25/40\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2911 - acc: 0.9237\n",
      "Epoch 00025: val_loss improved from 0.81966 to 0.66613, saving model to model.hdf5\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2911 - acc: 0.9237 - val_loss: 0.6661 - val_acc: 0.7143\n",
      "Epoch 26/40\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2078 - acc: 0.9492\n",
      "Epoch 00026: val_loss did not improve from 0.66613\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.2078 - acc: 0.9492 - val_loss: 1.2597 - val_acc: 0.5000\n",
      "Epoch 27/40\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1844 - acc: 0.9688\n",
      "Epoch 00027: val_loss did not improve from 0.66613\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.2632 - acc: 0.9237 - val_loss: 0.7114 - val_acc: 0.6429\n"
     ]
    }
   ],
   "source": [
    "#trainmodel\n",
    "import os\n",
    "#path=\"training_1/cp.ckpt\"\n",
    "#checkpoint_dir = os.path.dirname(path)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint=ModelCheckpoint(\"model.hdf5\",monitor='val_loss',verbose=True,save_best_only=True,mode='min')\n",
    "earlystop=EarlyStopping(monitor='val_acc',patience=10)\n",
    "hist=model.fit(embeddings_matrix_train,Y_train,epochs=40,batch_size=32,shuffle=True,validation_split=0.1,callbacks=[checkpoint,earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-db8079f3cb55>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "pred=model.predict_classes(embeddings_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3 3 0 2 2 3 2 4 2 1 2 0 3 1 3 3 2 3 2 0 0 4 0 3 3 2 0 1 2 0 1 0 2 0 1 2\n",
      " 3 4 2 2 0 0 1 2 0 3 2 3 1 3 3 3 2 3 0]\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2167 - acc: 0.6071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2166780233383179, 0.6071428656578064]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(embeddings_matrix_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never talk to me again\n",
      "üòû\n",
      "üç¥\n",
      "I am proud of your achievements\n",
      "üòÉ\n",
      "üòû\n",
      "It is the worst day in my life\n",
      "üòû\n",
      "üòû\n",
      "Miss you so much\n",
      "‚ù§Ô∏è\n",
      "‚ù§Ô∏è\n",
      "food is life\n",
      "üç¥\n",
      "üòÉ\n",
      "I love you mum\n",
      "‚ù§Ô∏è\n",
      "üòÉ\n",
      "Stop saying bullshit\n",
      "üòû\n",
      "üòû\n",
      "congratulations on your acceptance\n",
      "üòÉ\n",
      "üòÉ\n",
      "The assignment is too long\n",
      "üòû\n",
      "üç¥\n",
      "I want to go play\n",
      "‚öæ\n",
      "üòÉ\n",
      "she did not answer my text\n",
      "üòû\n",
      "‚öæ\n",
      "Your stupidity has no limit\n",
      "üòû\n",
      "üòÉ\n",
      "how many points did he score\n",
      "‚öæ\n",
      "‚ù§Ô∏è\n",
      "my algorithm performs poorly\n",
      "üòû\n",
      "üòû\n",
      "I got approved\n",
      "üòÉ\n",
      "‚öæ\n",
      "Stop shouting at me\n",
      "üòû\n",
      "üòû\n",
      "Sounds like a fun plan ha ha\n",
      "üòÉ\n",
      "üòû\n",
      "no one likes him\n",
      "üòû\n",
      "üòÉ\n",
      "the game just finished\n",
      "‚öæ\n",
      "üòû\n",
      "I will celebrate soon\n",
      "üòÉ\n",
      "üòÉ\n",
      "So sad you are not coming\n",
      "üòû\n",
      "‚ù§Ô∏è\n",
      "She is my dearest love\n",
      "‚ù§Ô∏è\n",
      "‚ù§Ô∏è\n",
      "Good job\n",
      "üòÉ\n",
      "üç¥\n",
      "It was funny lol\n",
      "üòÉ\n",
      "‚ù§Ô∏è\n",
      "candy is life\n",
      "üòÉ\n",
      "üòû\n",
      "The chicago cubs won again\n",
      "‚öæ\n",
      "üòû\n",
      "I am hungry\n",
      "üç¥\n",
      "üòÉ\n",
      "I am so excited to see you after so long\n",
      "üòÉ\n",
      "‚ù§Ô∏è\n",
      "you did well on you exam\n",
      "üòÉ\n",
      "‚öæ\n",
      "lets brunch some day\n",
      "üç¥\n",
      "üòÉ\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print(' '.join(X_train[i]))\n",
    "    print(emoji.emojize(emoji_dictionary[str(np.argmax(Y_train[i]))]))\n",
    "    print(emoji.emojize(emoji_dictionary[str(pred[i])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.json\",\"w\") as file:\n",
    "    file.write(model.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Django Integration</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.json\",\"r\") as file:\n",
    "    model=model_from_json(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str=\"how are you doing?\"\n",
    "X=pd.Series([test_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_X=embedding_output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=model.predict_classes(emb_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you doing?\n",
      "‚öæ\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(X[0]))\n",
    "print(emoji.emojize(emoji_dictionary[str(p[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
